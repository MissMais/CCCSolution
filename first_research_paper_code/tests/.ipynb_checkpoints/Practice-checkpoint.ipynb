{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93019d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# sys.path.append('../python')\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fb2025b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ticker_dict, tickerSymbols = get_categorical_tickers()\n",
    "start=\"2010-01-01\"\n",
    "end=\"2019-12-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e7d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickeranalysis = tickerSymbols[0]\n",
    "# LSTM_1 = LSTM_Model(tickerSymbol = tickeranalysis, start = start, end = end, depth = 0, naive = True)\n",
    "# LSTM_1.full_workflow_and_plot()\n",
    "# plt.clf()\n",
    "# LSTM_1.plot_bot_decision()\n",
    "# plt.clf()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa25c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "2515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 99600.,  99710.,  99850., ..., 339650., 338920., 338750.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def get_ticker_values():\n",
    "\n",
    "tickerData = yf.Ticker(tickerSymbols[0])\n",
    "# tickerData\n",
    "tickerDf = yf.download(tickerSymbols[0], start=start, end=end)\n",
    "# tickerDf         #prints total downloaded data\n",
    "tickerDf = tickerDf['Adj Close']\n",
    "# tickerDf         #data of a single column\n",
    "data = tickerDf\n",
    "y = data.values            #returns numpy representation of dataframe\n",
    "print(len(y))\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2becb901",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train_test_split = 0.8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# def prepare_test_train(self):\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m training_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(y\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m*\u001b[39m \u001b[43mtrain_test_split\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(training_size)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m training_mean \u001b[38;5;241m=\u001b[39m y[:training_size]\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# get the average\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "train_test_split = 0.8\n",
    "# def prepare_test_train(self):\n",
    "\n",
    "training_size = int(y.size * train_test_split)\n",
    "# print(training_size)\n",
    "training_mean = y[:training_size].mean()  # get the average\n",
    "# print(training_mean)\n",
    "training_std = y[:training_size].std()  # a measure of how far away individual measurements \n",
    "# print(training_std)                   # tend to be from the mean value of a data set\n",
    "y = (y - training_mean) / training_std  # prep data, use mean and std to maintain distribution and ratios\n",
    "# print(len(y))\n",
    "# print(y)\n",
    "\n",
    "xtrain, ytrain = data_preprocess(y, 0, training_size, 60, forward_look = 1)\n",
    "# print(len(xtrain))\n",
    "# print(len(ytrain))\n",
    "xtest, ytest = data_preprocess(y, training_size, None, 60, forward_look = 1)\n",
    "print(len(xtest))\n",
    "print(len(ytest))\n",
    "xtrain.shape[-2:]\n",
    "# print(xtrain.shape[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d0b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(dataset, iStart, iEnd, sHistory, forward_look=1):\n",
    "\n",
    "# Preprocess the data to make either the test set or the train set\n",
    "\n",
    "        data = []\n",
    "        target = []\n",
    "        iStart += sHistory\n",
    "#         print(iStart)\n",
    "\n",
    "        if iEnd is None:\n",
    "            iEnd = len(dataset) - forward_look + 1\n",
    "        for i in range(iStart, iEnd):\n",
    "            indices = range(i - sHistory, i)  # set the order\n",
    "#             print(indices)\n",
    "            if forward_look > 1:\n",
    "                fwd_ind = range(i, i + forward_look)\n",
    "                fwd_entity = np.asarray([])\n",
    "                fwd_entity = np.append(fwd_entity, dataset[fwd_ind])\n",
    "            reshape_entity = np.asarray([])\n",
    "            reshape_entity = np.append(reshape_entity, dataset[\n",
    "                indices])  # Comment this out if there are multiple identifiers in the feature vector\n",
    "#             print(reshape_entity)\n",
    "            data.append(np.reshape(reshape_entity, (sHistory, 1)))\n",
    "            print(data)\n",
    "            if forward_look > 1:\n",
    "                target.append(np.reshape(fwd_entity, (forward_look, 1)))\n",
    "            else:\n",
    "                target.append(dataset[i])\n",
    "        data = np.array(data)\n",
    "#         print(data)\n",
    "        target = np.array(target)\n",
    "#         print(target)\n",
    "        return data,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec58957a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorSliceDataset' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     23\u001b[0m                        loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 25\u001b[0m \u001b[43mcreate_p_test_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# hist = model.fit(p_train, epochs = self.epochs, steps_per_epoch = self.steps_per_epoch,\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#           validation_data = self.p_test, validation_steps = self.validation_steps,\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#           verbose = self.verbose)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[69], line 10\u001b[0m, in \u001b[0;36mcreate_p_test_train\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         BUFFER_SIZE \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m      9\u001b[0m         p_train \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m]) \u001b[38;5;66;03m#the slices of an array in the form of objects \u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[43mp_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m())\n\u001b[0;32m     11\u001b[0m         p_train \u001b[38;5;241m=\u001b[39m p_train\u001b[38;5;241m.\u001b[39mcache()\u001b[38;5;241m.\u001b[39mshuffle(BUFFER_SIZE)\u001b[38;5;241m.\u001b[39mbatch(BATCH_SIZE, drop_remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mrepeat()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#         print(p_train)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TensorSliceDataset' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "depth = 0\n",
    "naive = True\n",
    "custom_loss = False\n",
    "# def model_LSTM(self):\n",
    "#         Create the stacked LSTM model and train it using the shuffled train set\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "# print(model)\n",
    "if naive:\n",
    "    model.add(tf.keras.layers.LSTM(20, input_shape = xtrain.shape[-2:]))\n",
    "else:\n",
    "    model.add(tf.keras.layers.LSTM(20, return_sequences=True, input_shape = xtrain.shape[-2:]))\n",
    "for i in range(depth):\n",
    "    model.add(tf.keras.layers.LSTM(20, return_sequences=True))\n",
    "if naive is False:\n",
    "    model.add(tf.keras.layers.LSTM(20))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "if custom_loss:\n",
    "    model.compile(optimizer='Adam',\n",
    "              loss=custom_loss_def, metrics=['mse','mae'])\n",
    "else:\n",
    "    model.compile(optimizer='Adam',\n",
    "                       loss='mse', metrics=['mse','mae'])\n",
    "\n",
    "create_p_test_train()\n",
    "# hist = model.fit(p_train, epochs = self.epochs, steps_per_epoch = self.steps_per_epoch,\n",
    "#           validation_data = self.p_test, validation_steps = self.validation_steps,\n",
    "#           verbose = self.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6de947a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "\n",
    "def create_p_test_train():\n",
    "        '''\n",
    "        Prepare shuffled train and test data\n",
    "        '''\n",
    "        BATCH_SIZE = batch_size\n",
    "        BUFFER_SIZE = y.size\n",
    "        p_train = tf.data.Dataset.from_tensor_slices((xtrain,ytrain)) #the slices of an array in the form of objects \n",
    "        p_train = p_train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
    "#         print(p_train)\n",
    "        p_test = tf.data.Dataset.from_tensor_slices((xtest, ytest))\n",
    "        p_test = p_test.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673d745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
